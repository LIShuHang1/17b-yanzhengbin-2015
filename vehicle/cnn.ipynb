{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ckpt_dir/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_dir/model.ckpt\n",
      "epoch 0, loss is 0.04568 validation accuracy 0.98\n",
      "epoch 20, loss is 0.178394 validation accuracy 0.98\n",
      "[1.0091743119266054, 1.0363636363636364, 1.0103092783505154, 0.98765432098765427, 1.0142857142857142, 0.97777777777777775, 0.98765432098765427, 1.0238095238095237, 1.0091743119266054, 1.0, 0.98941798941798942, 0.98529411764705888, 0.97959183673469385, 0.91836734693877553, 1.0061349693251533, 1.0256410256410255, 0.97560975609756095, 1.01, 1.0, 0.96363636363636362, 1.0, 1.0181818181818181, 1.0, 0.97297297297297303, 1.0212765957446808, 1.0277777777777777, 0.98701298701298701, 1.0, 1.0, 1.0, 1.0, 1.0029069767441861, 1.0, 1.0]\n",
      "0.979642870171\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "def read_data(bat_size):\n",
    "    reader = tf.TFRecordReader()\n",
    "\n",
    "    filename_queue = tf.train.string_input_producer([\"tfrecord/validation.tfrecords\"])\n",
    "\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "    labels = tf.cast(features['label'], tf.int32)\n",
    "    images = tf.decode_raw(features['image_raw'], tf.int64)\n",
    "    images = tf.reshape(images, shape=[48, 24, 1])\n",
    "#     images = tf.split(images,3,2)[0]\n",
    "#     images = tf.div(tf.cast(images, tf.float32), 255)\n",
    "    # 采用shuffle_batch\n",
    "    images_batch, labels_batch = tf.train.shuffle_batch([images, labels],\n",
    "                                                            batch_size=bat_size,\n",
    "                                                            num_threads=2,\n",
    "                                                            capacity=500,\n",
    "                                                            min_after_dequeue=450,\n",
    "                                                            shapes=([48,24, 1],[])\n",
    "                                                            )\n",
    "       \n",
    "        \n",
    "    \n",
    "    return images_batch, labels_batch\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def deepnn(x):\n",
    "     # input\n",
    "    with tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(x, [-1, 48, 24, 1])\n",
    "  # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "    with tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.tanh(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "  # Pooling layer - downsamples by 2X.\n",
    "    with tf.name_scope('pool1'):\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "    # second convolutional layer\n",
    "    with tf.name_scope(\"conv2\"):\n",
    "        W_conv2 = weight_variable([5,5,32,64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.tanh(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    \n",
    "    with tf.name_scope(\"pool2\"):\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    pool_size = tf.shape(h_pool2)\n",
    "    reshape = tf.reshape(h_pool2, [pool_size[0], -1])\n",
    "    \n",
    "  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "    with tf.name_scope('fc1'):\n",
    "        W_fc1 = weight_variable([9*3*64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "\n",
    "        h_fc1 = tf.nn.tanh(tf.matmul(reshape, W_fc1) + b_fc1)\n",
    "    \n",
    "  # Dropout - controls the complexity of the model, prevents co-adaptation of features.\n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        \n",
    "    # Fully connected layer 2    \n",
    "    with tf.name_scope('fc2'):\n",
    "        W_fc2 = weight_variable([1024, 34])\n",
    "        b_fc2 = bias_variable([34])\n",
    "\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    return y_conv, keep_prob\n",
    "\n",
    "\n",
    "\n",
    "# Import data\n",
    "images_batch, labels_batch = read_data(100)\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 48, 24, 1])\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 34])\n",
    "\n",
    "def onhot(lab):\n",
    "    y_label = np.zeros([100,34])\n",
    "    for i in range(100):\n",
    "        y_label[i,lab[i]] = 1\n",
    "    return y_label\n",
    "\n",
    "# Build the graph for the deep net\n",
    "y_conv, keep_prob = deepnn(x)\n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy =  tf.nn.softmax_cross_entropy_with_logits(labels=y_,\n",
    "                                                        logits=y_conv)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.name_scope('adam_optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "ckpt_dir = \"./ckpt_dir\"\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "    \n",
    "# saver\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    # 初始化变量\n",
    "    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    # save\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "    # train\n",
    "#     for epoch in range(14400):\n",
    "#         img, lab = sess.run([images_batch, labels_batch])\n",
    "\n",
    "#         labe = onhot(lab)\n",
    "\n",
    "#         my_loss, _, accuracy_validation = sess.run([cross_entropy, train_step, accuracy], feed_dict={x: img, y_: labe, keep_prob: 0.5})\n",
    "#         saver.save(sess,ckpt_dir + \"/model.ckpt\")\n",
    "#         if epoch % 20 == 0:\n",
    "#             print('epoch %d, loss is %g validation accuracy %g' % (\n",
    "#                 epoch,\n",
    "#                 float(my_loss),\n",
    "#                 float(accuracy_validation)))\n",
    "    score = 0\n",
    "    recall_num = np.zeros([34])\n",
    "    label_num = np.zeros([34])\n",
    "    result_num = []\n",
    "    for epoch in range(28):\n",
    "        img, lab = sess.run([images_batch, labels_batch])\n",
    "\n",
    "        labe = onhot(lab)\n",
    "\n",
    "        yy, my_loss, _, accuracy_validation = sess.run([y_conv, cross_entropy, train_step, accuracy], feed_dict={x: img, y_: labe, keep_prob: 1.0})\n",
    "        score += accuracy_validation\n",
    "        for i in range(yy.shape[0]):\n",
    "            recall_num[np.argmax(yy[i])] += 1\n",
    "            label_num[lab[i]] += 1\n",
    "        if epoch % 20 == 0:\n",
    "            \n",
    "            print('epoch %d, loss is %g validation accuracy %g' % (\n",
    "                epoch,\n",
    "                float(my_loss),\n",
    "                float(accuracy_validation)))\n",
    "    for i in range(34):\n",
    "        result = recall_num[i] / label_num[i]\n",
    "        result_num.append(result)\n",
    "    print(result_num)\n",
    "    real_score = score * 100 / 2800\n",
    "    print(real_score)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEWCAYAAACNJFuYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npMb/AUuis8VU6SCIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe58a7dac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num = result_num\n",
    "\n",
    "plt.bar(range(len(num)), num)\n",
    "\n",
    "plt.xlabel(\"label\")\n",
    "plt.ylabel(\"recall\")\n",
    "plt.title(\"recall\")\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
